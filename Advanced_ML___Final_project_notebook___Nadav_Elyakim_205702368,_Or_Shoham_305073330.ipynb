{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLu5QmNZoy41"
      },
      "source": [
        "# Advanced ML - final project notebook\n",
        "Nadav Elyakim 205702368, Or Shoham 305073330\\\n",
        "July 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSaTD5RVsKwe"
      },
      "source": [
        "## Reference papers\n",
        "\n",
        "Anchor paper -  [BUT-FIT at SemEval-2019 Task 7: Determining the Rumour Stance with Pre-Trained Deep Bidirectional Transformers](https://aclanthology.org/S19-2192.pdf)\n",
        "\n",
        "Anchor paper implemention - [github](https://github.com/MFajcik/RumourEval2019)\n",
        "\n",
        "SemEval 2019 - [summary of the whole competition](https://aclanthology.org/S19-2147.pdf). \n",
        "\n",
        "**This notebook uses the data after our preprocessing which we saved on git.**\n",
        "\n",
        "**In order to run this notebook from the beginning download the original data**  [link](https://figshare.com/articles/dataset/RumourEval_2019_data/8845580)\\\n",
        "**and update the path below to the path which contains the folder DATA_rumoureval2019**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Z0upRgV9GAIw"
      },
      "outputs": [],
      "source": [
        "############################################################################################################################################\n",
        "#----------------------- Defult - use data after preprocessing (from git) ------------------#\n",
        "MAIN_PATH = \"Final-project/\"\n",
        "preprocessing = False\n",
        "\n",
        "#----------------------- If data preprocessing needed --------------------------------------#\n",
        "#----------------------- Change main dir and preprocessing to False ------------------------#\n",
        "# for example:\n",
        "# MAIN_PATH = \"/content/drive/MyDrive/AML/Final_Project/\"\n",
        "# preprocessing = True\n",
        "#-------------------------------------------------------------------------------------------#\n",
        "############################################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD2DWpF4G6H8"
      },
      "source": [
        "## IMPORTS AND INSTALLATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF59kwUZwfhN"
      },
      "source": [
        "### Google drive + git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH7fjt6yFthS",
        "outputId": "18dd4013-e25f-45bb-9fbb-a4065a9a1e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "fatal: destination path 'Final-project' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!git clone https://github.com/NadavElyakim27/Advanced-ML-Final-project
.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMMq0tw6wlH4"
      },
      "source": [
        "### Installations (pips)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dzZ8lSvuwz-H"
      },
      "outputs": [],
      "source": [
        "# Insalltions\n",
        "!pip install tweet-preprocessor==0.5.0 &> /dev/null\n",
        "!pip install torchtext==0.4.0 &> /dev/null\n",
        "!pip install pytorch_pretrained_bert==0.4.0 &> /dev/null \n",
        "!pip install vaderSentiment==3.2.1 &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70GPvYHGQVgX"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBmzWu27G8Kk",
        "outputId": "aee2ec4a-6d56-4205-9aa9-33a650a56f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n"
          ]
        }
      ],
      "source": [
        "# Imports:\n",
        "\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import csv\n",
        "import _io\n",
        "import _csv\n",
        "import math\n",
        "import nltk\n",
        "import json\n",
        "import yaml\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import socket\n",
        "import pickle\n",
        "import gensim\n",
        "import logging\n",
        "import requests\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from sklearn import metrics\n",
        "from itertools import chain\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn.modules.loss import _Loss\n",
        "from torchtext.data.dataset import Dataset\n",
        "from typing import Callable, Tuple, Dict, List\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from pytorch_pretrained_bert.modeling import PreTrainedBertModel\n",
        "from collections import defaultdict, Callable, Counter, OrderedDict\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from pytorch_pretrained_bert import BertModel, BertAdam, BertTokenizer\n",
        "from torchtext.data import BucketIterator, Iterator, Example, Batch, Field, RawField\n",
        "\n",
        "# nltk downloads\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True) \n",
        "nltk.download('averaged_perceptron_tagger', quiet=True) \n",
        "if preprocessing:\n",
        "  sys.path.append(os.path.abspath(MAIN_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HELPER FUNCTIONS"
      ],
      "metadata": {
        "id": "C5kwD0Dgya12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------IMPORTS AND INSTALLATIONS----------------------------------------#\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import nltk\n",
        "import spacy\n",
        "import math\n",
        "import socket\n",
        "import torch\n",
        "import datetime\n",
        "import logging\n",
        "import logging.config\n",
        "import yaml\n",
        "import string\n",
        "import warnings\n",
        "import numpy as np\n",
        "import preprocessor as twitter_preprocessor\n",
        "from copy import deepcopy\n",
        "from nltk.corpus import stopwords\n",
        "from spacy.symbols import PUNCT, SYM, ADJ, CCONJ, NUM, DET, ADV, ADP, VERB, NOUN, PROPN, PART, PRON, ORTH\n",
        "from collections import Iterable\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
        "map_stance_label_to_s = { 0: \"support\",1: \"comment\", 2: \"deny\", 3: \"query\"}\n",
        "map_s_to_label_stance = {y: x for x, y in map_stance_label_to_s.items()}\n",
        "\n",
        "\n",
        "#------------------------------------CREATE BRACNCHES FROM TREE---------------------------------------#\n",
        "def tree2branches(root):\n",
        "    \n",
        "    node = root\n",
        "    parent_tracker = []\n",
        "    parent_tracker.append(root)\n",
        "    branch = []\n",
        "    branches = []\n",
        "    i = 0\n",
        "    siblings = None\n",
        "    while True:\n",
        "        node_name = list(node.keys())[i]\n",
        "        branch.append(node_name)\n",
        "        first_child = list(node.values())[i]\n",
        "        if first_child != []:  \n",
        "            node = first_child  \n",
        "            parent_tracker.append(node)\n",
        "            siblings = list(first_child.keys())\n",
        "            i = 0  \n",
        "        else:\n",
        "            branches.append(deepcopy(branch))\n",
        "            if siblings is not None:\n",
        "                i = siblings.index(node_name) \n",
        "                while i + 1 >= len(siblings):\n",
        "                    if node is parent_tracker[0]:  \n",
        "                        return branches\n",
        "                    del parent_tracker[-1]\n",
        "                    del branch[-1]\n",
        "                    node = parent_tracker[-1] \n",
        "                    node_name = branch[-1]\n",
        "                    siblings = list(node.keys())\n",
        "                    i = siblings.index(node_name)\n",
        "                i = i + 1  \n",
        "                del branch[-1]\n",
        "            else:\n",
        "                return branches\n",
        "\n",
        "#------------------------------------PREPROCESS TEXT----------------------------------------#\n",
        "nlp = None\n",
        "punctuation = list(string.punctuation) + [\"``\"]\n",
        "stopWords = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text, opts, nlpengine=None, lang='en_core_web_sm', special_tags=[\"<pad>\", \"<eos>\"], use_tw_preprocessor=True):\n",
        "   \n",
        "\n",
        "    if use_tw_preprocessor:\n",
        "        text = re.sub(r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\", \"$URL$\",text.strip())\n",
        "        twitter_preprocessor.set_options('mentions')\n",
        "        text = twitter_preprocessor.tokenize(text)\n",
        "        \n",
        "    if nlpengine is None:\n",
        "        global nlp\n",
        "        if nlp is None:\n",
        "            nlp = spacy.load(lang)\n",
        "            nlp.add_pipe('sentencizer')\n",
        "            for x in ['URL', 'MENTION', 'HASHTAG', 'RESERVED', 'EMOJI', 'SMILEY', 'NUMBER', ]:\n",
        "                nlp.tokenizer.add_special_case(f'${x}$', [{ORTH: f'${x}$'}])\n",
        "        nlpengine = nlp\n",
        "        \n",
        "    processed_chunk = \"\"\n",
        "    doc = nlpengine(text)\n",
        "    doclen = 0\n",
        "    \n",
        "    for sentence in doc.sents:\n",
        "        for w in sentence:\n",
        "            word = \"_\".join(w.text.split())\n",
        "            if word.isspace() or word == \"\":\n",
        "                continue\n",
        "            if opts.remove_stop_words and word.lower() in stopWords:\n",
        "                continue\n",
        "            if opts.remove_puncuation and word in punctuation:\n",
        "                continue\n",
        "            if opts.lemmatize_words:\n",
        "                output = w.lemma_ if w.lemma_ != '-PRON-' else w.lower_\n",
        "            else:\n",
        "                output = word\n",
        "            if opts.to_lowercase:\n",
        "                output = output.lower()\n",
        "            if opts.replace_nums and output.replace('.', '', 1).isdigit():\n",
        "                output = opts.num_replacement\n",
        "            output = output.replace(\"n't\", \"not\")\n",
        "            doclen += 1\n",
        "            processed_chunk += \"%s \" % (output)\n",
        "\n",
        "        if opts.add_eos:\n",
        "            doclen += 1\n",
        "            processed_chunk += opts.eos + \"\\n\"\n",
        "        else:\n",
        "            processed_chunk += \"\\n\"\n",
        "\n",
        "    processed_chunk = processed_chunk.strip()\n",
        "    \n",
        "    return processed_chunk\n",
        "\n",
        "\n",
        "#------------------------------------TRANSFORM FEATURE TO DICT----------------------------------------#\n",
        "def transform_feature_dict(thread_feature_dict, conversation, feature_set):\n",
        "    \n",
        "\n",
        "    thread_features_array = []\n",
        "    thread_features_dict = []\n",
        "    thread_stance_labels = []\n",
        "    clean_branches = []\n",
        "\n",
        "    branches = conversation['branches']\n",
        "\n",
        "    for branch in branches:\n",
        "        branch_rep = []\n",
        "        branch_rep_dicts = []\n",
        "        # contains ids for tweets\n",
        "        clb = []\n",
        "        branch_stance_lab = []\n",
        "        for twid in branch:\n",
        "            if twid in thread_feature_dict.keys():\n",
        "                tweet_rep, tweet_rep_dict = dict_to_array_and_dict(thread_feature_dict[twid], feature_set)\n",
        "                branch_rep.append(tweet_rep)\n",
        "                branch_rep_dicts.append(tweet_rep_dict)\n",
        "\n",
        "                # if it is source tweet\n",
        "                if twid == branch[0]:\n",
        "                    # if it is labelled\n",
        "                    if 'label' in list(conversation['source'].keys()):\n",
        "                        branch_stance_lab.append(convert_label(\n",
        "                            conversation['source']['label']))\n",
        "                    clb.append(twid)\n",
        "                else:\n",
        "                    for r in conversation['replies']:\n",
        "                        if r['id_str'] == twid:\n",
        "                            if 'label' in list(r.keys()):\n",
        "                                branch_stance_lab.append(\n",
        "                                    convert_label(r['label']))\n",
        "                            clb.append(twid)\n",
        "        if branch_rep != []:\n",
        "            branch_rep = np.asarray(branch_rep)\n",
        "            branch_stance_lab = np.asarray(branch_stance_lab)\n",
        "            thread_features_array.append(branch_rep)\n",
        "            thread_features_dict.append(branch_rep_dicts)\n",
        "            thread_stance_labels.append(branch_stance_lab)\n",
        "            clean_branches.append(clb)\n",
        "\n",
        "    return thread_features_array, thread_features_dict, thread_stance_labels, clean_branches\n",
        "\n",
        "\n",
        "#------------------------------------TRANSFORM DICT TO ARRAY----------------------------------------#\n",
        "def dict_to_array(feature_dict, feature_set):\n",
        "\n",
        "    tweet_rep = []\n",
        "    for feature_name in feature_set:\n",
        "\n",
        "        if np.isscalar(feature_dict[feature_name]):\n",
        "            tweet_rep.append(feature_dict[feature_name])\n",
        "        else:\n",
        "            tweet_rep.extend(feature_dict[feature_name])\n",
        "    tweet_rep = np.asarray(tweet_rep)\n",
        "    return tweet_rep\n",
        "\n",
        "\n",
        "#------------------------------------TRANSFORM DICT TO ARRAY AND DICT----------------------------------------#\n",
        "def dict_to_array_and_dict(feature_dict, feature_set):\n",
        "\n",
        "    tweet_rep = []\n",
        "    tweet_rep_d = dict()\n",
        "    for feature_name in feature_set:\n",
        "        tweet_rep_d[feature_name] = feature_dict[feature_name]\n",
        "        if np.isscalar(feature_dict[feature_name]):\n",
        "            tweet_rep.append(feature_dict[feature_name])\n",
        "        else:\n",
        "            tweet_rep.extend(feature_dict[feature_name])\n",
        "\n",
        "    tweet_rep = np.asarray(tweet_rep)\n",
        "    return tweet_rep, tweet_rep_d\n",
        "\n",
        "\n",
        "#------------------------------------EXSTRACT THREAD FEATURES----------------------------------------#\n",
        "def extract_thread_features(conversation):\n",
        "    \n",
        "    feature_dict = {}\n",
        "    tw = conversation['source']\n",
        "    tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+', '', tw['text'].lower()))\n",
        "    otherthreadtweets = ''\n",
        "    for response in conversation['replies']:\n",
        "        otherthreadtweets += ' ' + response['text']\n",
        "    otherthreadtokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+', '', otherthreadtweets.lower()))\n",
        "    raw_txt = tw['text']\n",
        "    feature_dict['raw_text'] = raw_txt\n",
        "    feature_dict['spacy_processed_text']= preprocess_text(raw_txt, initopts())\n",
        "                              \n",
        "    return feature_dict\n",
        "\n",
        "#------------------------------------EXSTRACT THREAD FEATURES AND HIS REPLIES ----------------------------------------#\n",
        "def extract_thread_features_incl_response(conversation):\n",
        "\n",
        "    source_features = extract_thread_features(conversation)\n",
        "    source_features['issource'] = 1\n",
        "    srctokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+', '', conversation['source']['text'].lower()))\n",
        "    fullthread_featdict = {}\n",
        "    fullthread_featdict[conversation['source']['id_str']] = source_features\n",
        "\n",
        "    for tw in conversation['replies']:\n",
        "        feature_dict = {}\n",
        "        feature_dict['issource'] = 0\n",
        "        tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+', '',tw['text'].lower()))\n",
        "        otherthreadtweets = ''\n",
        "        otherthreadtweets += conversation['source']['text']\n",
        "\n",
        "        for response in conversation['replies']:\n",
        "            otherthreadtweets += ' ' + response['text']\n",
        "\n",
        "        otherthreadtokens = nltk.word_tokenize(re.sub( r'([^\\s\\w]|_)+', '',otherthreadtweets.lower()))\n",
        "        branches = tree2branches(conversation['structure'])\n",
        "\n",
        "        for branch in branches:\n",
        "            if tw['id_str'] in branch:\n",
        "                if branch.index(tw['id_str']) - 1 == 0:\n",
        "                    prevtokens = srctokens\n",
        "                else:\n",
        "                    prev_id = branch[branch.index(tw['id_str']) - 1]\n",
        "                    # Find conversation text for the id\n",
        "                    for ptw in conversation['replies']:\n",
        "                        if ptw['id_str'] == prev_id:\n",
        "                            prevtokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','', ptw['text'].lower()))\n",
        "                            break\n",
        "            else:\n",
        "                prevtokens = []\n",
        "            break\n",
        "\n",
        "        raw_txt = tw['text']\n",
        "        feature_dict['raw_text'] = raw_txt\n",
        "        feature_dict['spacy_processed_text'] = preprocess_text(raw_txt, initopts())\n",
        "        feature_dict['src_usr_hasurl'] = 0\n",
        "        fullthread_featdict[tw['id_str']] = feature_dict\n",
        "        \n",
        "    return fullthread_featdict\n",
        "\n",
        "            \n",
        "#------------------------------------GET CLASS WEIGHTS---------------------------------------#\n",
        "def get_class_weights(examples: Iterable, label_field_name: str, classes: int) -> torch.FloatTensor:\n",
        "\n",
        "    arr = torch.zeros(classes)\n",
        "    for e in examples:\n",
        "        arr[int(getattr(e, label_field_name))] += 1\n",
        "    arrmax = arr.max().expand(classes)\n",
        "    \n",
        "    return arrmax / arr\n",
        "\n",
        "#------------------------------------LOAD TRUE LABELS---------------------------------------#\n",
        "def load_true_labels(MAIN_PATH = MAIN_PATH):\n",
        "\n",
        "    PATH_TO_TRAIN_DATA = MAIN_PATH + \"DATA_rumoureval2019/rumoureval-2019-training-data\"\n",
        "    PATH_TO_TEST_LABEL = MAIN_PATH + \"DATA_rumoureval2019/final-eval-key.json\"\n",
        "\n",
        "    tweet_label_dict = {}\n",
        "    path_dev = os.path.join(PATH_TO_TRAIN_DATA, \"dev-key.json\")\n",
        "    with open(path_dev, 'r') as f:\n",
        "        dev_key = json.load(f)\n",
        "\n",
        "    path_train = os.path.join(PATH_TO_TRAIN_DATA, \"train-key.json\")\n",
        "    with open(path_train, 'r') as f:\n",
        "        train_key = json.load(f)\n",
        "\n",
        "    with open(PATH_TO_TEST_LABEL, 'r') as f:\n",
        "        test_key = json.load(f)\n",
        "\n",
        "    tweet_label_dict['dev'] = dev_key['subtaskaenglish']\n",
        "    tweet_label_dict['train'] = train_key['subtaskaenglish']\n",
        "    tweet_label_dict['test'] = test_key['subtaskaenglish']\n",
        "\n",
        "    return tweet_label_dict\n",
        "\n",
        "\n",
        "#------------------------------------HELPER FUNCTIONS---------------------------------------#\n",
        "def initopts():\n",
        "    \n",
        "    o = DotDict()\n",
        "    o.stopwords_file = \"\"\n",
        "    o.remove_puncuation = False\n",
        "    o.remove_stop_words = False\n",
        "    o.lemmatize_words = False\n",
        "    o.num_replacement = \"[NUM]\"\n",
        "    o.to_lowercase = False\n",
        "    o.replace_nums = False  # Nums are important, since rumour may be lying about count\n",
        "    o.eos = \"[EOS]\"\n",
        "    o.add_eos = True\n",
        "    o.returnNERvector = True\n",
        "    o.returnDEPvector = True\n",
        "    o.returnbiglettervector = True\n",
        "    o.returnposvector = True\n",
        "    \n",
        "    return o\n",
        "\n",
        "#------------------------------------#\n",
        "class DotDict(dict):\n",
        "\n",
        "    def __getattr__(self, key):\n",
        "        return self[key]\n",
        "    \n",
        "    def __setattr__(self, key, val):\n",
        "        if key in self.__dict__:\n",
        "            self.__dict__[key] = val\n",
        "        else:\n",
        "            self[key] = val\n",
        "            \n",
        "    def __getstate__(self):\n",
        "        pass\n",
        "    \n",
        "    def __setstate__(self, state):\n",
        "        pass\n",
        "\n",
        "#------------------------------------#\n",
        "def totext(batch, vocab, batch_first=True, remove_specials=False, check_for_zero_vectors=True):\n",
        "    \n",
        "    textlist = []\n",
        "    if not batch_first:\n",
        "        batch = batch.transpose(0, 1)\n",
        "    for ex in batch:\n",
        "        if remove_specials:\n",
        "            textlist.append(\n",
        "                ' '.join(\n",
        "                    [vocab.itos[ix.item()] for ix in ex\n",
        "                     if ix != vocab.stoi[\"<pad>\"] and ix != vocab.stoi[\"<eos>\"]]))\n",
        "        else:\n",
        "            if check_for_zero_vectors:\n",
        "                text = []\n",
        "                for ix in ex:\n",
        "                    if ix != vocab.stoi[\"<pad>\"] and ix != vocab.stoi[\"<eos>\"] \\\n",
        "                            and vocab.vectors[ix.item()].equal(vocab.vectors[vocab.stoi[\"<unk>\"]]):\n",
        "                        text.append(f\"[OOV]{vocab.itos[ix.item()]}\")\n",
        "                    else:\n",
        "                        text.append(vocab.itos[ix.item()])\n",
        "                textlist.append(' '.join(text))\n",
        "            else:\n",
        "                textlist.append(' '.join([vocab.itos[ix.item()] for ix in ex]))\n",
        "                \n",
        "    return textlist\n",
        "\n",
        "#------------------------------------#\n",
        "def count_parameters(model):\n",
        "    \n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "#------------------------------------#\n",
        "def convert_label(label):\n",
        "    if label == \"support\":\n",
        "        return (0)\n",
        "    elif label == \"comment\":\n",
        "        return (1)\n",
        "    elif label == \"deny\":\n",
        "        return (2)\n",
        "    elif label == \"query\":\n",
        "        return (3)\n",
        "    else:\n",
        "        print(label)\n",
        "\n",
        "#------------------------------------#\n",
        "def all_folders_in_folder(path):\n",
        "    list_all_folders_in = sorted(os.listdir(path))\n",
        "    folders_list = [i for i in list_all_folders_in if i[0] != '.']\n",
        "    finall_list = folders_list\n",
        "    return finall_list\n",
        "\n",
        "#------------------------------------#\n",
        "def save_file(file, path, name):\n",
        "  f = open(f'{path}{name}.pkl',\"wb\")\n",
        "  pickle.dump(file,f)\n",
        "  f.close()\n",
        "\n",
        "#------------------------------------#\n",
        "def read_file(path, name):\n",
        "  file_to_read = open(f'{path}{name}.pkl', \"rb\")\n",
        "  loaded_dictionary = pickle.load(file_to_read)\n",
        "  return loaded_dictionary\n"
      ],
      "metadata": {
        "id": "1ntIY1e-ymIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3789d86d-1d8b-4ed6-cb5a-27205a28cac3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BekAJE8dF0sM"
      },
      "source": [
        "## Data LOAD AND PREPROCESSING "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQt_jZk9TuRT"
      },
      "source": [
        "### LOADERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8T418ohhzM-"
      },
      "source": [
        "#### load test data - twitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1nd_fICwOmBH"
      },
      "outputs": [],
      "source": [
        "def load_test_data_twitter(MAIN_PATH=MAIN_PATH):\n",
        "\n",
        "    PATH_TO_TEST_TWITTER = MAIN_PATH + \"DATA_rumoureval2019/rumoureval-2019-test-data/twitter-en-test-data\"\n",
        "\n",
        "    # Initializing variables\n",
        "    train_dev_split = {}\n",
        "    train_dev_split['dev'], train_dev_split['train'], train_dev_split['test'] = [], [], []\n",
        "    tweet_label_dict = load_true_labels()\n",
        "    test_tweet_label = tweet_label_dict['test']\n",
        "\n",
        "    allconv = []\n",
        "\n",
        "    # Load folds\n",
        "    folds = all_folders_in_folder(PATH_TO_TEST_TWITTER)\n",
        "\n",
        "    # build conversations for tweet group\n",
        "    for fold in folds:\n",
        "        conversation = {}\n",
        "        path_to_tweets = os.path.join(PATH_TO_TEST_TWITTER, fold)\n",
        "        tweets_of_fold = all_folders_in_folder(path_to_tweets)\n",
        "\n",
        "        for source_tweet in tweets_of_fold:\n",
        "            flag = 0\n",
        "            replies_tweets = []\n",
        "            conversation['id'] = source_tweet\n",
        "            path_repl = path_to_tweets + '/' + source_tweet + '/replies'\n",
        "            reply_files =  all_folders_in_folder(path_repl)\n",
        "            flag = \"test\"\n",
        "\n",
        "            # Add replies data to conversations \n",
        "            if reply_files != []:\n",
        "                # iterate over json reply files\n",
        "                for repl_file in reply_files:\n",
        "                    with open(os.path.join(path_repl, repl_file)) as f:\n",
        "                        for line in f:\n",
        "                            tw = json.loads(line)\n",
        "                            tw['used'] = 0\n",
        "                            tw['set'] = flag\n",
        "                            tw['label'] = test_tweet_label[tw['id_str']]\n",
        "                            replies_tweets.append(tw)\n",
        "                conversation['replies'] = replies_tweets\n",
        "            \n",
        "            # Add source tweet data to conversation\n",
        "            path_src = path_to_tweets + '/' + source_tweet + '/source-tweet'\n",
        "            files_t = sorted(os.listdir(path_src))\n",
        "            with open(os.path.join(path_src, files_t[0])) as f:\n",
        "                for line in f:\n",
        "                    src = json.loads(line)\n",
        "                    src['used'] = 0\n",
        "                    scrcid = src['id_str']\n",
        "                    src['set'] = flag\n",
        "                    src['label'] = test_tweet_label[src['id_str']]\n",
        "            conversation['source'] = src\n",
        "            path_struct = path_to_tweets + '/' + source_tweet + '/structure.json'\n",
        "            with open(path_struct) as f:\n",
        "                for line in f:\n",
        "                    struct = json.loads(line)\n",
        "            if len(struct) > 1:\n",
        "                new_struct = {}\n",
        "                new_struct[source_tweet] = struct[source_tweet]\n",
        "                struct = new_struct\n",
        "            conversation['structure'] = struct\n",
        "\n",
        "            # Finally, add conversation and his structure to dataset\n",
        "            branches = tree2branches(conversation['structure'])\n",
        "            conversation['branches'] = branches\n",
        "            train_dev_split[flag].append(conversation.copy())\n",
        "            allconv.append(conversation.copy())\n",
        "\n",
        "    return train_dev_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hYymXwVh56V"
      },
      "source": [
        "#### load train data - twitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "azqLnZToBmG4"
      },
      "outputs": [],
      "source": [
        "def load_dataset_twitter(MAIN_PATH = MAIN_PATH):\n",
        "\n",
        "    PATH_TO_TRAIN_DATA = MAIN_PATH + \"DATA_rumoureval2019/rumoureval-2019-training-data\"\n",
        "       \n",
        "    tweet_label_dict = load_true_labels()\n",
        "    dev_tweet_label = tweet_label_dict['dev']\n",
        "    train_tweet_label =  tweet_label_dict['train']\n",
        "\n",
        "    # Load folds\n",
        "    path_to_folds = os.path.join(PATH_TO_TRAIN_DATA, 'twitter-english')\n",
        "    folds =  all_folders_in_folder(path_to_folds)\n",
        "\n",
        "    # Initializing variables\n",
        "    cvfolds, train_dev_split = {}, {}\n",
        "    train_dev_split['dev'], train_dev_split['train'], train_dev_split['test'] = [], [], []\n",
        "    allconv = []\n",
        "\n",
        "    # iterate over all tweet groups [reffered to as 'folds'] - charliehebdo etc...\n",
        "    for fold in folds:\n",
        "        conversation = {}\n",
        "        path_to_tweets = os.path.join(path_to_folds, fold)\n",
        "        tweets_of_fold = all_folders_in_folder(path_to_tweets)\n",
        "\n",
        "        # build conversations for tweet with all his replies\n",
        "        for source_tweet in tweets_of_fold:\n",
        "            flag = 0\n",
        "            replies_tweets = []\n",
        "            conversation['id'] = source_tweet\n",
        "            path_repl = path_to_tweets + '/' + source_tweet + '/replies'\n",
        "            reply_files =  all_folders_in_folder(path_repl)\n",
        "\n",
        "            # Add replies data to conversations \n",
        "            if reply_files != []:              \n",
        "                for repl_file in reply_files:  \n",
        "                    with open(os.path.join(path_repl, repl_file)) as f:\n",
        "                        for line in f:\n",
        "                            tw = json.loads(line)\n",
        "                            tw['used'] = 0\n",
        "                            # check if tweet belongs to dev fold or train\n",
        "                            if tw['id_str'] in dev_tweet_label.keys():\n",
        "                                tw['set'] = 'dev'\n",
        "                                tw['label'] = dev_tweet_label[tw['id_str']]\n",
        "                                if flag == 'train':\n",
        "                                    print(\"The tree is split between sets\", source_tweet)\n",
        "                                flag = 'dev'\n",
        "                            elif tw['id_str'] in train_tweet_label.keys():\n",
        "                                tw['set'] = 'train'\n",
        "                                tw['label'] = train_tweet_label[tw['id_str']]\n",
        "                                if flag == 'dev':\n",
        "                                    print(\"The tree is split between sets\", source_tweet)\n",
        "                                flag = 'train'\n",
        "                            else:\n",
        "                                print(\"Tweet was not found! ID: \", source_tweet)\n",
        "                            replies_tweets.append(tw)\n",
        "                conversation['replies'] = replies_tweets\n",
        "            else:\n",
        "              flag = 'train'\n",
        "\n",
        "            # Add source tweet data to conversation\n",
        "            path_src = path_to_tweets + '/' + source_tweet + '/source-tweet'\n",
        "            files_t = sorted(os.listdir(path_src))\n",
        "            with open(os.path.join(path_src, files_t[0])) as f:\n",
        "                for line in f:\n",
        "                    src = json.loads(line)\n",
        "                    src['used'] = 0\n",
        "                    scrcid = src['id_str']\n",
        "                    src['set'] = flag\n",
        "                    src['label'] = tweet_label_dict[flag][scrcid]\n",
        "            conversation['source'] = src\n",
        "            path_struct = path_to_tweets + '/' + source_tweet + '/structure.json'\n",
        "            with open(path_struct) as f:\n",
        "                for line in f:\n",
        "                    struct = json.loads(line)\n",
        "            if len(struct) > 1:\n",
        "                # I had to alter the structure of this conversation\n",
        "                if source_tweet == '553480082996879360':\n",
        "                    new_struct = {}\n",
        "                    new_struct[source_tweet] = struct[source_tweet]\n",
        "                    new_struct[source_tweet]['553495625527209985'] = struct['553485679129534464']['553495625527209985']\n",
        "                    new_struct[source_tweet]['553495937432432640'] = struct['553490097623269376']['553495937432432640']\n",
        "                    struct = new_struct\n",
        "                else:\n",
        "                    new_struct = {}\n",
        "                    new_struct[source_tweet] = struct[source_tweet]\n",
        "                    struct = new_struct\n",
        "\n",
        "            # Finally, add conversation and his structure to dataset\n",
        "            conversation['structure'] = struct\n",
        "            branches = tree2branches(conversation['structure'])\n",
        "            conversation['branches'] = branches\n",
        "            train_dev_split[flag].append(conversation.copy())\n",
        "            allconv.append(conversation.copy())\n",
        "\n",
        "        cvfolds[fold] = allconv\n",
        "        allconv = []\n",
        "\n",
        "    return train_dev_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwJx2u3f1Hnk"
      },
      "source": [
        "#### load test data - reddit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Nb1JACnMyadI"
      },
      "outputs": [],
      "source": [
        "def load_test_data_reddit(MAIN_PATH = MAIN_PATH):\n",
        "    \n",
        "    PATH_TO_TEST_REDDIT = MAIN_PATH + \"DATA_rumoureval2019/rumoureval-2019-test-data/reddit-test-data\"       \n",
        "\n",
        "    tweet_label_dict = load_true_labels()\n",
        "    test_tweet_label = tweet_label_dict['test']\n",
        "    count = 0\n",
        "    # Initializing variables\n",
        "    train_dev_split = {}\n",
        "    train_dev_split['dev'], train_dev_split['train'], train_dev_split['test'] = [], [], []\n",
        "    \n",
        "    # Load conversations\n",
        "    conversations_ids = all_folders_in_folder(PATH_TO_TEST_REDDIT)\n",
        "\n",
        "    for id in conversations_ids:\n",
        "        conversation = {}\n",
        "        conversation['id'] = id\n",
        "        path_src = PATH_TO_TEST_REDDIT + '/' + id + '/source-tweet'\n",
        "        source_file = all_folders_in_folder(path_src)\n",
        "        \n",
        "        # Add source tweet data to conversation\n",
        "        with open(os.path.join(path_src, source_file[0])) as f:\n",
        "            for line in f:\n",
        "                src = json.loads(line)\n",
        "                src['text'] = src['data']['children'][0]['data']['title']\n",
        "                src['user'] = src['data']['children'][0]['data']['author']               \n",
        "                src['id_str'] = source_file[0][:-5]\n",
        "                src['used'] = 0\n",
        "                src['setA'] = 'test'\n",
        "                src['label'] = test_tweet_label[src['id_str']]\n",
        "                count+=1\n",
        "                conversation['source'] = src\n",
        "\n",
        "        # Add replies data to conversations \n",
        "        replies_tweets = []\n",
        "        path_repl = PATH_TO_TEST_REDDIT + '/' + id + '/replies'\n",
        "        reply_files =  all_folders_in_folder(path_repl)\n",
        "        if reply_files != []:\n",
        "          for repl_file in reply_files:\n",
        "              with open(os.path.join(path_repl, repl_file)) as f:\n",
        "                  for line in f:\n",
        "                      tw = json.loads(line)\n",
        "                      if 'body' in list(tw['data'].keys()):\n",
        "                          tw['text'] = tw['data']['body']\n",
        "                          tw['user'] = tw['data']['author']\n",
        "                          tw['id_str'] = repl_file[:-5]\n",
        "                          tw['used'] = 0\n",
        "                          tw['setA'] = 'test'\n",
        "                          tw['label'] = test_tweet_label[tw['id_str']]\n",
        "                          replies_tweets.append(tw)\n",
        "                          count+=1\n",
        "                      else:\n",
        "                          tw['text'] = ''\n",
        "                          tw['user'] = ''\n",
        "                          tw['used'] = 0\n",
        "                          tw['id_str'] = repl_file[:-5]\n",
        "                          tw['setA'] = 'test'\n",
        "                          tw['label'] = test_tweet_label[tw['id_str']]\n",
        "                          replies_tweets.append(tw)\n",
        "                          count+=1\n",
        "          conversation['replies'] = replies_tweets\n",
        "\n",
        "        path_struct = PATH_TO_TEST_REDDIT + '/' + id + '/structure.json'\n",
        "        with open(path_struct, 'r') as f:\n",
        "            struct = json.load(f)\n",
        "            conversation['structure'] = struct\n",
        "            branches = tree2branches(conversation['structure'])\n",
        "            conversation['branches'] = branches\n",
        "\n",
        "        # Finally, add conversation and his structure to dataset\n",
        "        train_dev_split['test'].append(conversation)\n",
        "        \n",
        "    print(count)\n",
        "    return train_dev_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJHA-t_I05W7"
      },
      "source": [
        "#### load train data - reddit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KHv5dlK406GJ"
      },
      "outputs": [],
      "source": [
        "def load_dataset_reddit(MAIN_PATH=MAIN_PATH):\n",
        "\n",
        "    PATH_TO_TRAIN_DATA = MAIN_PATH + \"DATA_rumoureval2019/rumoureval-2019-training-data\"\n",
        "\n",
        "    tweet_label_dict = load_true_labels()\n",
        "    dev_tweet_label = tweet_label_dict['dev']\n",
        "    train_tweet_label =  tweet_label_dict['train']\n",
        "\n",
        "    # Initializing variables\n",
        "    train_dev_split = {}\n",
        "    train_dev_split['dev'], train_dev_split['train'], train_dev_split['test'] = [], [], []\n",
        "    \n",
        "    # Load conversations - reddit-training-data\n",
        "    path = os.path.join(PATH_TO_TRAIN_DATA, \"reddit-training-data\")\n",
        "    conversations_ids = all_folders_in_folder(path)\n",
        "\n",
        "    for id in conversations_ids:\n",
        "        conversation = {}\n",
        "        conversation['id'] = id\n",
        "        path_src = path + '/' + id + '/source-tweet'\n",
        "        source_file = all_folders_in_folder(path_src)\n",
        "\n",
        "        # Add source tweet data to conversation\n",
        "        with open(os.path.join(path_src, source_file[0])) as f:\n",
        "            for line in f:\n",
        "                src = json.loads(line)\n",
        "                src['text'] = src['data']['children'][0]['data']['title']\n",
        "                src['user'] = src['data']['children'][0]['data']['author']\n",
        "                src['id_str'] =  source_file[0][:-5]\n",
        "                src['used'] = 0\n",
        "                if src['id_str'] in list(dev_tweet_label.keys()):\n",
        "                    src['setA'] = 'dev'\n",
        "                    src['label'] = dev_tweet_label[src['id_str']]\n",
        "                elif src['id_str'] in list(train_tweet_label.keys()):\n",
        "                    src['setA'] = 'train'\n",
        "                    src['label'] = train_tweet_label[src['id_str']]\n",
        "                else:\n",
        "                    print(\"Post was not found! Task A, Post ID: \", src['id_str'])\n",
        "                conversation['source'] = src\n",
        "        path_struct = path + '/' + id + '/structure.json'\n",
        "        with open(path_struct, 'r') as f:\n",
        "            struct = json.load(f)\n",
        "            conversation['structure'] = struct\n",
        "            branches = tree2branches(conversation['structure'])\n",
        "            conversation['branches'] = branches\n",
        " \n",
        "        # Add replies data to conversations \n",
        "        replies_tweets = []\n",
        "        path_repl = path + '/' + id + '/replies'\n",
        "        reply_files = all_folders_in_folder(path_repl)\n",
        "        for repl_file in reply_files:\n",
        "            with open(os.path.join(path_repl, repl_file)) as f:\n",
        "                for line in f:\n",
        "                    tw = json.loads(line)\n",
        "                    if 'body' in list(tw['data'].keys()):\n",
        "                        tw['text'] = tw['data']['body']\n",
        "                        tw['user'] = tw['data']['author']\n",
        "                        tw['id_str'] =  repl_file[:-5]\n",
        "                        tw['used'] = 0\n",
        "                        if tw['id_str'] in list(dev_tweet_label.keys()):\n",
        "                            tw['setA'] = 'dev'\n",
        "                            tw['label'] = dev_tweet_label[tw['id_str']]\n",
        "                        elif tw['id_str'] in list(train_tweet_label.keys()):\n",
        "                            tw['setA'] = 'train'\n",
        "                            tw['label'] = train_tweet_label[tw['id_str']]\n",
        "                        else:\n",
        "                            print(\"Post was not found! Task A, Reply ID: \", tw['id_str'])\n",
        "                        replies_tweets.append(tw)\n",
        "                    else:\n",
        "                        tw['text'] = ''\n",
        "                        tw['user'] = ''\n",
        "                        tw['used'] = 0\n",
        "                        tw['id_str'] = repl_file[:-5]\n",
        "                        if tw['id_str'] in list(dev_tweet_label.keys()):\n",
        "                            tw['setA'] = 'dev'\n",
        "                            tw['label'] = dev_tweet_label[tw['id_str']]\n",
        "                        elif tw['id_str'] in list(train_tweet_label.keys()):\n",
        "                            tw['setA'] = 'train'\n",
        "                            tw['label'] = train_tweet_label[tw['id_str']]\n",
        "                        else:\n",
        "                            print(\"Post was not found! Task A, Reply ID: \", tw['id_str'])\n",
        "                        replies_tweets.append(tw)\n",
        "\n",
        "        conversation['replies'] = replies_tweets\n",
        "        train_dev_split['train'].append(conversation)\n",
        "\n",
        "    # Load conversations - reddit-dev-data\n",
        "    path = os.path.join(PATH_TO_TRAIN_DATA, \"reddit-dev-data\")\n",
        "    conversations_ids = all_folders_in_folder(path)\n",
        "\n",
        "    for id in conversations_ids:\n",
        "        conversation = {}\n",
        "        conversation['id'] = id\n",
        "        path_src = path + '/' + id + '/source-tweet'\n",
        "        source_file = all_folders_in_folder(path_src)\n",
        "\n",
        "        # Add source tweet data to conversation\n",
        "        with open(os.path.join(path_src, source_file[0])) as f:\n",
        "            for line in f:\n",
        "                src = json.loads(line)\n",
        "                src['text'] = src['data']['children'][0]['data']['title']\n",
        "                src['user'] = src['data']['children'][0]['data']['author']\n",
        "                src['id_str'] =  source_file[0][:-5]\n",
        "                src['used'] = 0\n",
        "                if src['id_str'] in list(dev_tweet_label.keys()):\n",
        "                    src['setA'] = 'dev'\n",
        "                    src['label'] = dev_tweet_label[src['id_str']]\n",
        "                elif src['id_str'] in list(train_tweet_label.keys()):\n",
        "                    src['setA'] = 'train'\n",
        "                    src['label'] = train_tweet_label[src['id_str']]\n",
        "                else:\n",
        "                    print(\"Post was not found! Task A, Post ID: \", src['id_str'])\n",
        "                conversation['source'] = src\n",
        "        path_struct = path + '/' + id + '/structure.json'\n",
        "        with open(path_struct, 'r') as f:\n",
        "            struct = json.load(f)\n",
        "            conversation['structure'] = struct\n",
        "            branches = tree2branches(conversation['structure'])\n",
        "            conversation['branches'] = branches\n",
        " \n",
        "        # Add replies data to conversations \n",
        "        replies_tweets = []\n",
        "        path_repl = path + '/' + id + '/replies'\n",
        "        reply_files = all_folders_in_folder(path_repl)\n",
        "        for repl_file in reply_files:\n",
        "            with open(os.path.join(path_repl, repl_file)) as f:\n",
        "                for line in f:\n",
        "                    tw = json.loads(line)\n",
        "                    if 'body' in list(tw['data'].keys()):\n",
        "                        tw['text'] = tw['data']['body']\n",
        "                        tw['user'] = tw['data']['author']\n",
        "                        tw['id_str'] =  repl_file[:-5]\n",
        "                        tw['used'] = 0\n",
        "                        if tw['id_str'] in list(dev_tweet_label.keys()):\n",
        "                            tw['setA'] = 'dev'\n",
        "                            tw['label'] = dev_tweet_label[tw['id_str']]\n",
        "                        elif tw['id_str'] in list(train_tweet_label.keys()):\n",
        "                            tw['setA'] = 'train'\n",
        "                            tw['label'] = train_tweet_label[tw['id_str']]\n",
        "                        else:\n",
        "                            print(\"Post was not found! Task A, Reply ID: \", tw['id_str'])\n",
        "                        replies_tweets.append(tw)\n",
        "                    else:\n",
        "                        tw['text'] = ''\n",
        "                        tw['user'] = ''\n",
        "                        tw['used'] = 0\n",
        "                        tw['id_str'] = repl_file[:-5]\n",
        "                        if tw['id_str'] in list(dev_tweet_label.keys()):\n",
        "                            tw['setA'] = 'dev'\n",
        "                            tw['label'] = dev_tweet_label[tw['id_str']]\n",
        "                        elif tw['id_str'] in list(train_tweet_label.keys()):\n",
        "                            tw['setA'] = 'train'\n",
        "                            tw['label'] = train_tweet_label[tw['id_str']]\n",
        "                        else:\n",
        "                            print(\"Post was not found! Task A, Reply ID: \", tw['id_str'])\n",
        "                        replies_tweets.append(tw)\n",
        "\n",
        "        conversation['replies'] = replies_tweets\n",
        "        train_dev_split['dev'].append(conversation)\n",
        "\n",
        "    return train_dev_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyWzjM1OMbpU"
      },
      "source": [
        "#### Main Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vXWBjUNKMOAx"
      },
      "outputs": [],
      "source": [
        "def main_data_loader(MAIN_PATH=MAIN_PATH):\n",
        "\n",
        "  path  = MAIN_PATH + \"saved_data/\"\n",
        "\n",
        "  # Load the data if not save allready\n",
        "  if not os.path.exists(f'{path}loaded_data.pkl'):\n",
        "\n",
        "    test_data_twitter = load_test_data_twitter()    \n",
        "    save_file(test_data_twitter, path, 'test_data_twitter')\n",
        "    test_data_reddit = load_test_data_reddit()\n",
        "    save_file(test_data_reddit, path, 'test_data_reddit')\n",
        "    train_data_twitter = load_dataset_twitter()\n",
        "    save_file(train_data_twitter, path, 'train_data_twitter')\n",
        "    train_data_reddit = load_dataset_reddit()\n",
        "    save_file(train_data_reddit, path, 'train_data_reddit')\n",
        "    print('Load datasets Successfully')\n",
        "    print('\\nConnects the datasets to one dataset')\n",
        "\n",
        "    data = train_data_twitter\n",
        "    data[\"test\"] = test_data_twitter[\"test\"]\n",
        "    data[\"train\"].extend(train_data_reddit['train'])\n",
        "    data['dev'].extend(train_data_reddit['dev'])\n",
        "    data[\"test\"].extend(test_data_reddit['test'])\n",
        "    \n",
        "    save_file(data, path, 'loaded_data')\n",
        "    print('Connects finished - Dataset save Successfully')\n",
        "\n",
        "  # If data allready load and saved - read the saved file\n",
        "  else:\n",
        "    data = read_file(path, 'loaded_data')\n",
        "\n",
        "  # Print \n",
        "  for key in data.keys():\n",
        "    count_examples = 0\n",
        "    count_discussions = 0\n",
        "    for discussions in data[key]:\n",
        "      count_discussions +=1\n",
        "      count_examples = 1 + len(discussions['replies']) + count_examples\n",
        "    print(f'{key}: {count_examples} examples in {count_discussions} discussions')\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWr7SYSy1xA-"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fMmwBgum5byc"
      },
      "outputs": [],
      "source": [
        "def save_final_data(fold, fold_features_dict, tweet_ids, fold_stance_labels, path):\n",
        "\n",
        "    print(f\"Writing dataset to {path}\")\n",
        "    jsonformat = {\"Examples\": []}\n",
        "    cnt = 0\n",
        "    already_known_tweetids = set()\n",
        "    for fold_idx in tqdm(range(len(fold_features_dict))):\n",
        "        e = fold_features_dict[fold_idx]\n",
        "        tweet_ids_branch = tweet_ids[fold_idx]\n",
        "        branch_labels = fold_stance_labels[fold_idx].tolist()\n",
        "        for idx in range(len(e)):\n",
        "            if tweet_ids_branch[idx] in already_known_tweetids:\n",
        "                continue\n",
        "            else:\n",
        "                already_known_tweetids.add(tweet_ids_branch[idx])\n",
        "            example = {\n",
        "                \"id\": cnt,\n",
        "                \"branch_id\": f\"{fold_idx}.{idx}\",\n",
        "                \"tweet_id\": tweet_ids_branch[idx],\n",
        "                \"stance_label\": branch_labels[idx],\n",
        "                \"raw_text\": e[idx][\"raw_text\"],\n",
        "                \"raw_text_prev\": e[idx - 1][\"raw_text\"] if idx - 1 > -1 else \"\",\n",
        "                \"raw_text_src\": e[0][\"raw_text\"] if idx - 1 > -1 else \"\",\n",
        "                \"issource\": e[idx][\"issource\"],\n",
        "                \"spacy_processed_text\": e[idx][\"spacy_processed_text\"],\n",
        "                \"spacy_processed_text_prev\": e[idx - 1][\"spacy_processed_text\"] if idx - 1 > -1 else \"\",\n",
        "                \"spacy_processed_text_src\": e[0][\"spacy_processed_text\"] if idx - 1 > -1 else \"\"\n",
        "            }\n",
        "            cnt += 1\n",
        "            example = {i: (v if type(v) is not np.ndarray else v.tolist())\n",
        "                        for i, v in example.items()}\n",
        "            jsonformat[\"Examples\"].append(example)\n",
        "\n",
        "    json.dump(jsonformat, open(path, \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cMe2Uwzmvk4I"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing(data, MAIN_PATH = MAIN_PATH):\n",
        "\n",
        "  path = MAIN_PATH + \"saved_data\"\n",
        "  feature_set = ['issource','raw_text','spacy_processed_text']\n",
        "      \n",
        "  all_fold_features = {} \n",
        "\n",
        "  for fold in data.keys(): # /train/dev/test\n",
        "      fold_features, fold_features_dict, tweet_ids, fold_stance_labels, conv_ids = [], [] , [], [],[]\n",
        "      print(fold)\n",
        "\n",
        "      for conversation in tqdm(data[fold]):\n",
        "\n",
        "          # extract features for source and replies\n",
        "          thread_feature_dict = extract_thread_features_incl_response(conversation)\n",
        "          thread_features_array, thread_features_dict, thread_stance_labels, branches = transform_feature_dict(thread_feature_dict, conversation, feature_set=feature_set)\n",
        "\n",
        "          # Add to all fold \n",
        "          fold_features.extend(thread_features_array)\n",
        "          fold_features_dict.extend(thread_features_dict)\n",
        "          fold_stance_labels.extend(thread_stance_labels)\n",
        "          tweet_ids.extend(branches)\n",
        "\n",
        "      path_fold = os.path.join(path, f\"{fold}.json\")\n",
        "      save_final_data(fold, fold_features_dict, tweet_ids, fold_stance_labels, path_fold)\n",
        "\n",
        "      all_fold_features[f'{fold}'] = fold_features_dict\n",
        "\n",
        "  return all_fold_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHyWsJaw3Kxl"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DULhpkzNLjn4"
      },
      "outputs": [],
      "source": [
        "class Dataset_BERT(Dataset):\n",
        "    \"\"\"\n",
        "    Creates dataset, where each example is composed --[CLS] source post, previous post [SEP] choice_1 [SEP]--\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path, fields, tokenizer, max_length = 512, **kwargs):\n",
        "\n",
        "        max_length = max_length - 3  # didn't count special tokens\n",
        "        sentiment_analyser = SentimentIntensityAnalyzer()\n",
        "        with open(path) as dataf:\n",
        "            data_json = json.load(dataf)\n",
        "            examples = []\n",
        "            for example in data_json[\"Examples\"]:\n",
        "                make_ids = lambda x: tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x))\n",
        "                text = make_ids(example[\"spacy_processed_text\"])\n",
        "                prev = make_ids(example[\"spacy_processed_text_prev\"])\n",
        "                src = make_ids(example[\"spacy_processed_text_src\"])\n",
        "                segment_A = src + prev\n",
        "                segment_B = text\n",
        "                text_ids = [tokenizer.vocab[\"[CLS]\"]] + segment_A + [tokenizer.vocab[\"[SEP]\"]] + segment_B + [tokenizer.vocab[\"[SEP]\"]]\n",
        "                if len(text_ids) > max_length:\n",
        "                    segment_A = segment_A[:max_length // 2]\n",
        "                    text_ids = [tokenizer.vocab[\"[CLS]\"]] + segment_A + [tokenizer.vocab[\"[SEP]\"]] + segment_B + [tokenizer.vocab[\"[SEP]\"]]\n",
        "                    if len(text_ids) > max_length:\n",
        "                        segment_B = segment_B[:max_length // 2]\n",
        "                        text_ids = [tokenizer.vocab[\"[CLS]\"]] + segment_A + [tokenizer.vocab[\"[SEP]\"]] + segment_B + [tokenizer.vocab[\"[SEP]\"]]\n",
        "\n",
        "                segment_ids = [0] * (len(segment_A) + 2) + [1] * (len(segment_B) + 1)\n",
        "                input_mask = [1] * len(segment_ids)\n",
        "\n",
        "                sentiment = sentiment_analyser.polarity_scores(example[\"raw_text\"])\n",
        "                example_list = [example[\"id\"], example[\"branch_id\"], example[\"tweet_id\"], example[\"stance_label\"],\"\\n-----------\\n\".join([example[\"raw_text_src\"], example[\"raw_text_prev\"], example[\"raw_text\"]]),example[\"issource\"], sentiment[\"pos\"], sentiment[\"neu\"], sentiment[\"neg\"]] + [text_ids, segment_ids, input_mask]\n",
        "                examples.append(Example.fromlist(example_list, fields))\n",
        "            super(Dataset_BERT, self).__init__(examples, fields, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_fields_for_text():\n",
        "        text_field = lambda: Field(use_vocab=False, batch_first=True, sequential=True, pad_token=0)\n",
        "        return [\n",
        "            ('id', RawField()),\n",
        "            ('branch_id', RawField()),\n",
        "            ('tweet_id', RawField()),\n",
        "            ('stance_label', Field(sequential=False, use_vocab=False, batch_first=True, is_target=True)),\n",
        "            ('raw_text', RawField()),\n",
        "            ('issource', Field(use_vocab=False, batch_first=True, sequential=False)),\n",
        "            ('sentiment_pos', Field(use_vocab=False, batch_first=True, sequential=False)),\n",
        "            ('sentiment_neu', Field(use_vocab=False, batch_first=True, sequential=False)),\n",
        "            ('sentiment_neg', Field(use_vocab=False, batch_first=True, sequential=False)),\n",
        "            ('text', text_field()),\n",
        "            ('type_mask', text_field()),\n",
        "            ('input_mask', text_field())]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertModelForStanceClassification(PreTrainedBertModel):\n",
        "\n",
        "    def __init__(self, config, classes=4):\n",
        "        super(BertModelForStanceClassification, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.last_layer = nn.Linear(config.hidden_size, classes)\n",
        "        self.apply(self.init_bert_weights)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def reinit(self, config):\n",
        "        self.dropout = nn.Dropout(config[\"hyperparameters\"][\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, batch):\n",
        "        _, pooled_output = self.bert(batch.text, batch.type_mask, batch.input_mask,\n",
        "                                     output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.last_layer(pooled_output)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "YqhO9SxKd1kX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vbDXhmvU8HP3"
      },
      "outputs": [],
      "source": [
        "class BERT_Framework():\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        self.config = config\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.config[\"variant\"], cache_dir=\"./.BERTcache\", do_lower_case=True)\n",
        "\n",
        "        # Load Data\n",
        "        fields = Dataset_BERT.prepare_fields_for_text()\n",
        "        self.train_data = Dataset_BERT(config[\"train_data\"], fields, self.tokenizer, max_length=config[\"hyperparameters\"][\"max_length\"])\n",
        "        self.dev_data = Dataset_BERT(config[\"dev_data\"], fields, self.tokenizer,max_length=config[\"hyperparameters\"][\"max_length\"])\n",
        "        self.test_data = Dataset_BERT(config[\"test_data\"], fields, self.tokenizer, max_length=config[\"hyperparameters\"][\"max_length\"])\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------------------- #\n",
        "\n",
        "    def fit(self, modelfunc: Callable) -> dict:\n",
        "        \"\"\"\n",
        "        Trains the model and executes early stopping\n",
        "        :param modelfunc: model constructor\n",
        "        :return Summary of model performance\n",
        "        \"\"\"\n",
        "\n",
        "        config = self.config\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f'Training on {device}\\n')\n",
        "\n",
        "        # Create Data Iterators\n",
        "        train_iter = BucketIterator(self.train_data, sort_key=lambda x: -len(x.text), sort=True, shuffle=False, batch_size=config[\"hyperparameters\"][\"batch_size\"], repeat=False, device=device)\n",
        "        dev_iter = BucketIterator(self.dev_data, sort_key=lambda x: -len(x.text), sort=True, shuffle=False, batch_size=config[\"hyperparameters\"][\"batch_size\"], repeat=False,device=device)\n",
        "        test_iter = BucketIterator(self.test_data, sort_key=lambda x: -len(x.text), sort=True, shuffle=False, batch_size=config[\"hyperparameters\"][\"batch_size\"], repeat=False,device=device)\n",
        "\n",
        "        # Create BERT model\n",
        "        print(f'Load pretrained \"bert-large-uncased\" model')\n",
        "        model = modelfunc.from_pretrained(\"bert-large-uncased\", cache_dir=\"./.BERTcache\").to(device)\n",
        "\n",
        "\n",
        "        print(f\"\\nTrain examples: {len(self.train_data.examples)}\\nValidation examples: {len(self.dev_data.examples)}\\nTest examples: {len(self.test_data.examples)}\")\n",
        "        print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\\n\")\n",
        "\n",
        "        optimizer = BertAdam(filter(lambda p: p.requires_grad, model.parameters()), lr=config[\"hyperparameters\"][\"learning_rate\"])\n",
        "\n",
        "        # Calculate weights for current data distribution:\n",
        "        weights = get_class_weights(self.train_data.examples, \"stance_label\", 4)\n",
        "        lossfunction = torch.nn.CrossEntropyLoss(weight=weights.to(device))\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Initialization counters:\n",
        "        best_val_loss, best_val_acc, best_val_F1 = math.inf, 0, 0\n",
        "        bestF1_testF1, bestF1_testacc, bestF1_test_F1s = 0, 0 , [0, 0, 0, 0]\n",
        "        start_time = time.time()\n",
        "        best_val_loss_epoch = -1\n",
        "        loss_memo, acc_memo = [], []\n",
        "         \n",
        "        # Train:\n",
        "        for epoch in range(config[\"hyperparameters\"][\"epochs\"]):\n",
        "            self.epoch = epoch\n",
        "            print(f'\\033[1mTraining Epoch {epoch+1}:\\033[0m')\n",
        "            train_loss, train_acc = self.train(model, lossfunction, optimizer, train_iter, config)\n",
        "            validation_loss, validation_acc, val_acc_per_level, val_F1, val_allF1s = self.validate(model, lossfunction, dev_iter, config, 'Val')\n",
        "            loss_memo.append(validation_loss)\n",
        "            acc_memo.append(validation_acc)\n",
        "            sorted_val_acc_pl = sorted(val_acc_per_level.items(), key=lambda x: int(x[0]))\n",
        "            if validation_loss < best_val_loss:\n",
        "                best_val_loss = validation_loss\n",
        "                best_val_loss_epoch = epoch\n",
        "            if validation_acc > best_val_acc:\n",
        "                best_val_acc = validation_acc\n",
        "            if val_F1 > best_val_F1:\n",
        "                best_val_F1 = val_F1\n",
        "                test_loss, bestF1_testacc, test_acc_per_level, bestF1_testF1, bestF1_test_F1s = self.validate(model, lossfunction, test_iter, config, 'Test')\n",
        "\n",
        "            # Print Epoch Info \n",
        "            print(f\"Training loss|acc: {train_loss:.6f}|{train_acc:.6f}\")\n",
        "            print(f\"Validation loss|acc|F1: {validation_loss:.6f}|{validation_acc:.6f}|{val_F1:.6f} - (Best {best_val_loss:.3f}|{best_val_acc:3f}|{best_val_F1:.3f}) \")\n",
        "            print(f\"\\033[4mBest test acc - {bestF1_testacc:.6f} | Best test F1 - {bestF1_testF1:.6f}\\033[0m\\n\")\n",
        "\n",
        "            if validation_loss > best_val_loss and epoch > best_val_loss_epoch + self.config[\"early_stop_after\"]:\n",
        "                print(\"Early stopping...\")\n",
        "                break\n",
        "\n",
        "        print(f'Finished after {(time.time() - start_time) / 60:.1f} minutes.\\n')\n",
        "\n",
        "        return {\n",
        "            \"best_loss\": best_val_loss,\n",
        "            \"best_acc\": best_val_acc,\n",
        "            \"best_F1\": best_val_F1,\n",
        "            \"bestACC_testACC\": bestF1_testacc,\n",
        "            \"bestF1_testF1\": bestF1_testF1,\n",
        "            \"test_bestF1_C1F1\": bestF1_test_F1s[0], \"test_bestF1_C2F1\": bestF1_test_F1s[1], \"test_bestF1_C3F1\": bestF1_test_F1s[2], \"test_bestF1_C4F1\": bestF1_test_F1s[3],\n",
        "            \"loss_memo\":loss_memo, \"acc_memo\": acc_memo}\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------------------- #            \n",
        "\n",
        "    def train(self, model, lossfunction, optimizer,train_iter, config):\n",
        "        \"\"\"\n",
        "        :param model: model inherited from torch.nn.Module\n",
        "        :param lossfunction:\n",
        "        :param optimizer:\n",
        "        :param train_iter:\n",
        "        :param config:\n",
        "        :return: train loss and train accuracy\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialization counters \n",
        "        examples_so_far = 0\n",
        "        train_loss = 0\n",
        "        total_correct = 0\n",
        "        N = 0\n",
        "        updated = False\n",
        "\n",
        "        # I case of gradient accumulalation, how often should gradient be updated\n",
        "        update_ratio = config[\"hyperparameters\"][\"true_batch_size\"] // config[\"hyperparameters\"][\"batch_size\"]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        for i, batch in enumerate(tqdm(train_iter, desc=\"Train: \")):\n",
        "            updated = False\n",
        "            pred_logits = model(batch)\n",
        "            loss = lossfunction(pred_logits, batch.stance_label) / update_ratio\n",
        "            loss.backward()\n",
        "            if (i + 1) % update_ratio == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                updated = True\n",
        "\n",
        "            # Update accumulators\n",
        "            train_loss += loss.item()\n",
        "            N += 1 if not hasattr(lossfunction, \"weight\") \\\n",
        "                else sum([lossfunction.weight[k].item() for k in batch.stance_label])\n",
        "            total_correct += self.calculate_correct(pred_logits, batch.stance_label)\n",
        "            examples_so_far += len(batch.stance_label)\n",
        "            \n",
        "         # Do the last step if needed with what has been accumulated\n",
        "        if not updated:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        return train_loss / N, total_correct / examples_so_far\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------------------- #\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, model, lossfunction, dev_iter, config, kind):\n",
        "        \"\"\"\n",
        "        :param model: model inherited from torch.nn.Module\n",
        "        :param lossfunction:\n",
        "        :param dev_iter:\n",
        "        :param config:\n",
        "        :return: validation loss, validation accuracy, validation accuracies per level, validation F1, per class F1s\n",
        "        \"\"\"\n",
        "\n",
        "        train_flag = model.training\n",
        "        model.eval()\n",
        "\n",
        "        # initialize accumulators \n",
        "        examples_so_far = 0\n",
        "        dev_loss = 0\n",
        "        total_correct = 0\n",
        "        N = 0\n",
        "        total_correct_per_level = Counter()\n",
        "        total_per_level = defaultdict(lambda: 0)\n",
        "        total_labels = []\n",
        "        total_preds = []\n",
        "\n",
        "        for i, batch in enumerate(tqdm(dev_iter, desc=f'{kind}: ')):\n",
        "            pred_logits = model(batch)\n",
        "            loss = lossfunction(pred_logits, batch.stance_label)\n",
        "            maxpreds, argmaxpreds = torch.max(F.softmax(pred_logits, -1), dim=1)\n",
        "\n",
        "            # compute branch statistics\n",
        "            branch_levels = [id.split(\".\", 1)[-1] for id in batch.branch_id]\n",
        "            for branch_depth in branch_levels: total_per_level[branch_depth] += 1\n",
        "            # compute correct and correct per branch depth\n",
        "            correct, correct_per_level = self.calculate_correct(pred_logits, batch.stance_label, levels=branch_levels)\n",
        "            total_correct += correct\n",
        "            total_correct_per_level += correct_per_level\n",
        "            examples_so_far += len(batch.stance_label)\n",
        "            dev_loss += loss.item()\n",
        "            N += 1 if not hasattr(lossfunction, \"weight\") \\\n",
        "                else sum([lossfunction.weight[k].item() for k in batch.stance_label])\n",
        "            total_preds += list(argmaxpreds.cpu().numpy())\n",
        "            total_labels += list(batch.stance_label.cpu().numpy())\n",
        "\n",
        "        loss, acc = dev_loss / N, total_correct / examples_so_far\n",
        "        total_acc_per_level = {depth: total_correct_per_level.get(depth, 0) / total for depth, total in total_per_level.items()}\n",
        "        F1 = metrics.f1_score(total_labels, total_preds, average=\"macro\").item()\n",
        "        allF1s = metrics.f1_score(total_labels, total_preds, average=None).tolist()\n",
        "\n",
        "        return loss, acc, total_acc_per_level, F1, allF1s\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------------------- #\n",
        "\n",
        "    def fit_multiple(self, modelfunc, trials = 11):\n",
        "        \"\"\"\n",
        "        Runs training multiple times until achieve anchor paper F!\n",
        "        \"\"\"\n",
        "\n",
        "        results = []\n",
        "        for i in range(trials):\n",
        "          print(f'\\n\\033[4m\\033[1mTrial number {i+1}:\\033[0m\\033[0m (lr={self.config[\"hyperparameters\"][\"learning_rate\"]})')\n",
        "          torch.manual_seed(random.randint(1, 1e8))\n",
        "          result = self.fit(modelfunc)\n",
        "          results.append(result)\n",
        "          print(result)\n",
        "          if result['bestF1_testF1'] >= 0.5670:\n",
        "            print(\"***************************************************************************************************\")\n",
        "            print(\"****************We achieve accuracy as in the anchor article, the training is stopped**************\")\n",
        "            print(\"***************************************************************************************************\")\n",
        "            break\n",
        "          # self.config[\"hyperparameters\"][\"learning_rate\"] = self.config[\"hyperparameters\"][\"learning_rate\"] + 0.000001\n",
        "\n",
        "        return results\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------------------- #\n",
        "\n",
        "    def calculate_correct(self, pred_logits, labels, levels=None):\n",
        "\n",
        "        preds = torch.argmax(pred_logits, dim=1)\n",
        "        correct_vec = preds == labels\n",
        "        if not levels:\n",
        "            return torch.sum(correct_vec).item()\n",
        "        else:\n",
        "            sums_per_level = defaultdict(lambda: 0)\n",
        "            for level, correct in zip(levels, correct_vec):\n",
        "                sums_per_level[level] += correct.item()\n",
        "        return torch.sum(correct_vec).item(), sums_per_level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADJisZMhG_RI"
      },
      "source": [
        "## RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qbJqoFV17XhN"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'cuda': True,\n",
        "    'train_data': MAIN_PATH + \"saved_data/train.json\",\n",
        "    'dev_data': MAIN_PATH + \"saved_data/dev.json\",\n",
        "    'test_data': MAIN_PATH + \"saved_data/test.json\",\n",
        "    'early_stop_after': 3,\n",
        "    'variant': 'bert-large-uncased',\n",
        "    'hyperparameters': \n",
        "        {'attention_probs_dropout_prob': 0.5,\n",
        "        'batch_size': 2,\n",
        "        'epochs': 90,\n",
        "        'hidden_act': 'gelu',\n",
        "        'hidden_dropout_prob': 0.1,\n",
        "        'hidden_size': 768,\n",
        "        'initializer_range': 0.02,\n",
        "        'intermediate_size': 3072,\n",
        "        'learning_rate': 3e-06,\n",
        "        'max_length': 200,\n",
        "        'max_position_embeddings': 512,\n",
        "        'num_attention_heads': 12,\n",
        "        'num_hidden_layers': 12,\n",
        "        'true_batch_size': 32,\n",
        "        'type_vocab_size': 2,\n",
        "        'vocab_size': 30522},\n",
        "        }\n",
        "\n",
        "if preprocessing:\n",
        "  loaded_data = main_data_loader()\n",
        "  all_data = data_preprocessing(loaded_data)\n",
        "\n",
        "# Create model:\n",
        "modelframework = BERT_Framework(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdTa_QNRq0Gq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deeb6fd3-0fcf-48bc-d964-b681525d0c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[4m\u001b[1mTrial number 1:\u001b[0m\u001b[0m (lr=1e-06)\n",
            "Training on cuda\n",
            "\n",
            "Load pretrained \"bert-large-uncased\" model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1248501532/1248501532 [00:30<00:00, 40377217.38B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train examples: 5217\n",
            "Validation examples: 1485\n",
            "Test examples: 1827\n",
            "Trainable parameters: 335145988\n",
            "\n",
            "\u001b[1mTraining Epoch 1:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   1%|          | 15/2609 [00:04<07:46,  5.56it/s]/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/optimization.py:132: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
            "Train: 100%|| 2609/2609 [04:47<00:00,  9.09it/s]\n",
            "Val: 100%|| 743/743 [00:26<00:00, 28.53it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 28.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.013493|0.701361\n",
            "Validation loss|acc|F1: 0.226329|0.729966|0.288132 - (Best 0.226|0.729966|0.288) \n",
            "\u001b[4mBest test acc - 0.619595 | Best test F1 - 0.265437\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 2:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:40<00:00,  9.29it/s]\n",
            "Val: 100%|| 743/743 [00:26<00:00, 28.48it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 28.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.012220|0.734330\n",
            "Validation loss|acc|F1: 0.198491|0.811448|0.328158 - (Best 0.198|0.811448|0.328) \n",
            "\u001b[4mBest test acc - 0.836891 | Best test F1 - 0.340395\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 3:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:40<00:00,  9.29it/s]\n",
            "Val: 100%|| 743/743 [00:26<00:00, 28.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.011785|0.743339\n",
            "Validation loss|acc|F1: 0.206335|0.789226|0.312677 - (Best 0.198|0.811448|0.328) \n",
            "\u001b[4mBest test acc - 0.836891 | Best test F1 - 0.340395\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 4:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:40<00:00,  9.29it/s]\n",
            "Val: 100%|| 743/743 [00:26<00:00, 28.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.011251|0.751581\n",
            "Validation loss|acc|F1: 0.201636|0.782492|0.302983 - (Best 0.198|0.811448|0.328) \n",
            "\u001b[4mBest test acc - 0.836891 | Best test F1 - 0.340395\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 5:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:40<00:00,  9.30it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.010544|0.764807\n",
            "Validation loss|acc|F1: 0.195821|0.798653|0.313917 - (Best 0.196|0.811448|0.328) \n",
            "\u001b[4mBest test acc - 0.836891 | Best test F1 - 0.340395\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 6:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:40<00:00,  9.31it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.60it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 28.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.009398|0.792409\n",
            "Validation loss|acc|F1: 0.190042|0.799327|0.346685 - (Best 0.190|0.811448|0.347) \n",
            "\u001b[4mBest test acc - 0.835249 | Best test F1 - 0.357388\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 7:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:38<00:00,  9.36it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 29.04it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 29.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.008095|0.827295\n",
            "Validation loss|acc|F1: 0.204733|0.758249|0.418394 - (Best 0.190|0.811448|0.418) \n",
            "\u001b[4mBest test acc - 0.799124 | Best test F1 - 0.401809\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 8:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:38<00:00,  9.38it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 29.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.007194|0.842630\n",
            "Validation loss|acc|F1: 0.331690|0.525253|0.300258 - (Best 0.190|0.811448|0.418) \n",
            "\u001b[4mBest test acc - 0.799124 | Best test F1 - 0.401809\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 9:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:36<00:00,  9.43it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 29.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.006588|0.857389\n",
            "Validation loss|acc|F1: 0.263334|0.645791|0.358409 - (Best 0.190|0.811448|0.418) \n",
            "\u001b[4mBest test acc - 0.799124 | Best test F1 - 0.401809\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 10:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:37<00:00,  9.42it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 29.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.006488|0.849339\n",
            "Validation loss|acc|F1: 0.249802|0.668687|0.384237 - (Best 0.190|0.811448|0.418) \n",
            "\u001b[4mBest test acc - 0.799124 | Best test F1 - 0.401809\u001b[0m\n",
            "\n",
            "Early stopping...\n",
            "Finished after 53.1 minutes.\n",
            "\n",
            "{'best_loss': 0.1900423250910042, 'best_acc': 0.8114478114478114, 'best_F1': 0.41839400874329513, 'bestACC_testACC': 0.7991242474001095, 'bestF1_testF1': 0.40180945252460054, 'test_bestF1_C1F1': 0.36416184971098264, 'test_bestF1_C2F1': 0.8958537381651974, 'test_bestF1_C3F1': 0.0, 'test_bestF1_C4F1': 0.34722222222222215, 'loss_memo': [0.2263287359215552, 0.19849059007836228, 0.20633509533993896, 0.20163632841044601, 0.19582061151921204, 0.1900423250910042, 0.20473330702442558, 0.33169008703373914, 0.2633343484893516, 0.24980159104123026], 'acc_memo': [0.7299663299663299, 0.8114478114478114, 0.7892255892255893, 0.7824915824915825, 0.7986531986531986, 0.7993265993265993, 0.7582491582491583, 0.5252525252525253, 0.6457912457912458, 0.6686868686868687]}\n",
            "\n",
            "\u001b[4m\u001b[1mTrial number 2:\u001b[0m\u001b[0m (lr=2e-06)\n",
            "Training on cuda\n",
            "\n",
            "Load pretrained \"bert-large-uncased\" model\n",
            "\n",
            "Train examples: 5217\n",
            "Validation examples: 1485\n",
            "Test examples: 1827\n",
            "Trainable parameters: 335145988\n",
            "\n",
            "\u001b[1mTraining Epoch 1:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:41<00:00,  9.28it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 29.07it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 29.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.013654|0.628139\n",
            "Validation loss|acc|F1: 0.231544|0.589899|0.358283 - (Best 0.232|0.589899|0.358) \n",
            "\u001b[4mBest test acc - 0.526546 | Best test F1 - 0.307982\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 2:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:36<00:00,  9.43it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 29.05it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 29.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.011269|0.728196\n",
            "Validation loss|acc|F1: 0.187033|0.773064|0.467286 - (Best 0.187|0.773064|0.467) \n",
            "\u001b[4mBest test acc - 0.775588 | Best test F1 - 0.414398\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 3:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:36<00:00,  9.43it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.86it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 29.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.009922|0.760015\n",
            "Validation loss|acc|F1: 0.154259|0.812121|0.503131 - (Best 0.154|0.812121|0.503) \n",
            "\u001b[4mBest test acc - 0.823207 | Best test F1 - 0.444721\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 4:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:40<00:00,  9.31it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.95it/s]\n",
            "Test: 100%|| 914/914 [00:32<00:00, 28.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.008141|0.797968\n",
            "Validation loss|acc|F1: 0.149036|0.791246|0.537810 - (Best 0.149|0.812121|0.538) \n",
            "\u001b[4mBest test acc - 0.818281 | Best test F1 - 0.477290\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 5:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:42<00:00,  9.23it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.005861|0.856431\n",
            "Validation loss|acc|F1: 0.178937|0.721212|0.496619 - (Best 0.149|0.812121|0.538) \n",
            "\u001b[4mBest test acc - 0.818281 | Best test F1 - 0.477290\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 6:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:39<00:00,  9.34it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.79it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 28.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.003877|0.912018\n",
            "Validation loss|acc|F1: 0.167889|0.785185|0.545866 - (Best 0.149|0.812121|0.546) \n",
            "\u001b[4mBest test acc - 0.824302 | Best test F1 - 0.503547\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 7:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:42<00:00,  9.23it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.002779|0.935979\n",
            "Validation loss|acc|F1: 0.181533|0.765657|0.537590 - (Best 0.149|0.812121|0.546) \n",
            "\u001b[4mBest test acc - 0.824302 | Best test F1 - 0.503547\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 8:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:39<00:00,  9.33it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.001915|0.957063\n",
            "Validation loss|acc|F1: 0.205902|0.764983|0.521894 - (Best 0.149|0.812121|0.546) \n",
            "\u001b[4mBest test acc - 0.824302 | Best test F1 - 0.503547\u001b[0m\n",
            "\n",
            "Early stopping...\n",
            "Finished after 43.4 minutes.\n",
            "\n",
            "{'best_loss': 0.14903583685819285, 'best_acc': 0.8121212121212121, 'best_F1': 0.5458658849950038, 'bestACC_testACC': 0.8243021346469622, 'bestF1_testF1': 0.5035465068880199, 'test_bestF1_C1F1': 0.3971119133574007, 'test_bestF1_C2F1': 0.9089726918075424, 'test_bestF1_C3F1': 0.27272727272727276, 'test_bestF1_C4F1': 0.43537414965986393, 'loss_memo': [0.23154425005308177, 0.1870328616208769, 0.15425878392264686, 0.14903583685819285, 0.1789370928276781, 0.16788918067937225, 0.18153271801008014, 0.20590163255230656], 'acc_memo': [0.5898989898989899, 0.773063973063973, 0.8121212121212121, 0.7912457912457912, 0.7212121212121212, 0.7851851851851852, 0.7656565656565657, 0.764983164983165]}\n",
            "\n",
            "\u001b[4m\u001b[1mTrial number 3:\u001b[0m\u001b[0m (lr=3e-06)\n",
            "Training on cuda\n",
            "\n",
            "Load pretrained \"bert-large-uncased\" model\n",
            "\n",
            "Train examples: 5217\n",
            "Validation examples: 1485\n",
            "Test examples: 1827\n",
            "Trainable parameters: 335145988\n",
            "\n",
            "\u001b[1mTraining Epoch 1:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:43<00:00,  9.21it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.80it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 28.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.012938|0.657466\n",
            "Validation loss|acc|F1: 0.226723|0.724579|0.392380 - (Best 0.227|0.724579|0.392) \n",
            "\u001b[4mBest test acc - 0.732895 | Best test F1 - 0.326546\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 2:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:39<00:00,  9.33it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.79it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 28.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.011326|0.719571\n",
            "Validation loss|acc|F1: 0.175049|0.791246|0.478597 - (Best 0.175|0.791246|0.479) \n",
            "\u001b[4mBest test acc - 0.786535 | Best test F1 - 0.454009\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 3:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:41<00:00,  9.27it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.79it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 28.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.009651|0.769791\n",
            "Validation loss|acc|F1: 0.164234|0.815488|0.485930 - (Best 0.164|0.815488|0.486) \n",
            "\u001b[4mBest test acc - 0.833607 | Best test F1 - 0.470149\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 4:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:39<00:00,  9.34it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.007752|0.806977\n",
            "Validation loss|acc|F1: 0.190235|0.749495|0.427563 - (Best 0.164|0.815488|0.486) \n",
            "\u001b[4mBest test acc - 0.833607 | Best test F1 - 0.470149\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 5:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:39<00:00,  9.33it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.78it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 28.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.005680|0.852597\n",
            "Validation loss|acc|F1: 0.172248|0.784512|0.552697 - (Best 0.164|0.815488|0.553) \n",
            "\u001b[4mBest test acc - 0.816092 | Best test F1 - 0.544943\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 6:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:39<00:00,  9.34it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.004182|0.893617\n",
            "Validation loss|acc|F1: 0.209027|0.721212|0.476208 - (Best 0.164|0.815488|0.553) \n",
            "\u001b[4mBest test acc - 0.816092 | Best test F1 - 0.544943\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 7:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:39<00:00,  9.34it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.002965|0.929461\n",
            "Validation loss|acc|F1: 0.197190|0.815488|0.542059 - (Best 0.164|0.815488|0.553) \n",
            "\u001b[4mBest test acc - 0.816092 | Best test F1 - 0.544943\u001b[0m\n",
            "\n",
            "Early stopping...\n",
            "Finished after 37.8 minutes.\n",
            "\n",
            "{'best_loss': 0.16423423872793955, 'best_acc': 0.8154882154882155, 'best_F1': 0.5526968863608556, 'bestACC_testACC': 0.8160919540229885, 'bestF1_testF1': 0.5449434279678338, 'test_bestF1_C1F1': 0.43636363636363634, 'test_bestF1_C2F1': 0.9023658780406532, 'test_bestF1_C3F1': 0.33789954337899547, 'test_bestF1_C4F1': 0.5031446540880504, 'loss_memo': [0.22672333244161247, 0.1750492451865763, 0.16423423872793955, 0.1902350616145886, 0.17224814741146313, 0.20902748341375116, 0.19719049520367324], 'acc_memo': [0.7245791245791245, 0.7912457912457912, 0.8154882154882155, 0.7494949494949495, 0.7845117845117845, 0.7212121212121212, 0.8154882154882155]}\n",
            "\n",
            "\u001b[4m\u001b[1mTrial number 4:\u001b[0m\u001b[0m (lr=4e-06)\n",
            "Training on cuda\n",
            "\n",
            "Load pretrained \"bert-large-uncased\" model\n",
            "\n",
            "Train examples: 5217\n",
            "Validation examples: 1485\n",
            "Test examples: 1827\n",
            "Trainable parameters: 335145988\n",
            "\n",
            "\u001b[1mTraining Epoch 1:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:43<00:00,  9.19it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.76it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 28.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.013019|0.686218\n",
            "Validation loss|acc|F1: 0.342202|0.152189|0.199514 - (Best 0.342|0.152189|0.200) \n",
            "\u001b[4mBest test acc - 0.143404 | Best test F1 - 0.161691\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 2:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:39<00:00,  9.34it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.79it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 28.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.011920|0.717654\n",
            "Validation loss|acc|F1: 0.181679|0.767677|0.449712 - (Best 0.182|0.767677|0.450) \n",
            "\u001b[4mBest test acc - 0.781609 | Best test F1 - 0.444401\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 3:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:39<00:00,  9.34it/s]\n",
            "Val: 100%|| 743/743 [00:26<00:00, 27.72it/s]\n",
            "Test: 100%|| 914/914 [00:32<00:00, 28.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.010238|0.749856\n",
            "Validation loss|acc|F1: 0.148685|0.816835|0.507412 - (Best 0.149|0.816835|0.507) \n",
            "\u001b[4mBest test acc - 0.833060 | Best test F1 - 0.506231\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 4:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:39<00:00,  9.35it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.77it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 28.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.008390|0.791259\n",
            "Validation loss|acc|F1: 0.159574|0.805387|0.518927 - (Best 0.149|0.816835|0.519) \n",
            "\u001b[4mBest test acc - 0.840175 | Best test F1 - 0.556597\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 5:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:39<00:00,  9.34it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.005509|0.851447\n",
            "Validation loss|acc|F1: 0.175665|0.789899|0.487170 - (Best 0.149|0.816835|0.519) \n",
            "\u001b[4mBest test acc - 0.840175 | Best test F1 - 0.556597\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 6:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:39<00:00,  9.33it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.003425|0.904735\n",
            "Validation loss|acc|F1: 0.198138|0.792593|0.516782 - (Best 0.149|0.816835|0.519) \n",
            "\u001b[4mBest test acc - 0.840175 | Best test F1 - 0.556597\u001b[0m\n",
            "\n",
            "\u001b[1mTraining Epoch 7:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|| 2609/2609 [04:39<00:00,  9.33it/s]\n",
            "Val: 100%|| 743/743 [00:25<00:00, 28.74it/s]\n",
            "Test: 100%|| 914/914 [00:31<00:00, 28.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss|acc: 0.002277|0.941537\n",
            "Validation loss|acc|F1: 0.216965|0.812121|0.545432 - (Best 0.149|0.816835|0.545) \n",
            "\u001b[4mBest test acc - 0.833060 | Best test F1 - 0.575698\u001b[0m\n",
            "\n",
            "Early stopping...\n",
            "Finished after 38.4 minutes.\n",
            "\n",
            "{'best_loss': 0.14868519400913557, 'best_acc': 0.8168350168350168, 'best_F1': 0.5454322994379162, 'bestACC_testACC': 0.8330596606458676, 'bestF1_testF1': 0.5756975838946322, 'test_bestF1_C1F1': 0.4426877470355731, 'test_bestF1_C2F1': 0.9043250327653998, 'test_bestF1_C3F1': 0.35064935064935066, 'test_bestF1_C4F1': 0.6051282051282052, 'loss_memo': [0.34220201882851753, 0.1816791262268448, 0.14868519400913557, 0.15957380110128164, 0.17566500531398083, 0.19813816313749982, 0.21696529118366617], 'acc_memo': [0.15218855218855218, 0.7676767676767676, 0.8168350168350168, 0.8053872053872054, 0.7898989898989899, 0.7925925925925926, 0.8121212121212121]}\n",
            "***************************************************************************************************\n",
            "****************We achieve accuracy as in the anchor article, the training is stopped**************\n",
            "***************************************************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Fit model:\n",
        "results = modelframework.fit_multiple(BertModelForStanceClassification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge-6rCVZO1lc"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_result = results[-1]\n",
        "losses = best_result.pop(\"loss_memo\")\n",
        "acces = best_result.pop(\"acc_memo\")\n",
        "plt.plot(losses)\n",
        "plt.plot(acces)\n",
        "plt.title('Loss and Acc per epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss', 'acc'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "print(f'\\033[1mResults:\\033[0m')\n",
        "pd.Series(best_result).multiply(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "lpdSHtuR7foY",
        "outputId": "632bcc47-2fc1-45de-ccef-5bceeb16727b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mResults:\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "best_loss           14.868519\n",
              "best_acc            81.683502\n",
              "best_F1             54.543230\n",
              "bestACC_testACC     83.305966\n",
              "bestF1_testF1       57.569758\n",
              "test_bestF1_C1F1    44.268775\n",
              "test_bestF1_C2F1    90.432503\n",
              "test_bestF1_C3F1    35.064935\n",
              "test_bestF1_C4F1    60.512821\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wD2DWpF4G6H8",
        "jF59kwUZwfhN",
        "bMMq0tw6wlH4",
        "70GPvYHGQVgX",
        "C5kwD0Dgya12",
        "BekAJE8dF0sM",
        "tQt_jZk9TuRT",
        "N8T418ohhzM-",
        "2hYymXwVh56V",
        "KwJx2u3f1Hnk",
        "wJHA-t_I05W7",
        "IyWzjM1OMbpU",
        "JHyWsJaw3Kxl"
      ],
      "name": "Advanced ML | Final project notebook  | Nadav Elyakim 205702368, Or Shoham 305073330.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
